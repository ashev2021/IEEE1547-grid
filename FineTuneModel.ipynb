{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "# Training Open source model for OpenDER IEEE1547 document\n",
        "\n",
        "(IEEE 1547-2018 Open Source Distributed Energy Resource (OpenDER) Model: Version 2.2)\n",
        "\n",
        "\n",
        "please run the pip installs as they appear below, and look the other way if you get an error!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "# With much thanks to Islam S. for identifying that there was a missing import!\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-7B\"\n",
        "#BASE_MODEL = \"meta-llama/Meta-Llama-3-8B\"\n",
        "PROJECT_NAME = \"NeWgrid\"\n",
        "HF_USER = \"ArezoSh2021\" # your HF name here!\n",
        "\n",
        "# Data\n",
        "\n",
        "DATASET_NAME = \"ArezoSh2021/ieee1547-qa\"\n",
        "# Or just use the one I've uploaded\n",
        "# DATASET_NAME = \"ed-donner/pricer-data\"\n",
        "MAX_SEQUENCE_LENGTH = 182\n",
        "\n",
        "# Run name for saving the model in the hub\n",
        "\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# Hyperparameters for QLoRA\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "LORA_DROPOUT = 0.1\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "# Hyperparameters for Training\n",
        "\n",
        "EPOCHS = 1 # you can do more epochs if you wish, but only 1 is needed - more is probably overkill\n",
        "BATCH_SIZE = 4 # on an A100 box this can go up to 16\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WARMUP_RATIO = 0.03\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "\n",
        "# Admin config - note that SAVE_STEPS is how often it will upload to the hub\n",
        "# Small steps because of GPU\n",
        "\n",
        "STEPS = 5\n",
        "SAVE_STEPS = 10\n",
        "LOG_TO_WANDB = True\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HUB_MODEL_NAME"
      ],
      "metadata": {
        "id": "QyHOj-c4FmkM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9e40d2ac-76b0-4485-bdda-52b3cd9b6a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ArezoSh2021/NeWgrid-2025-08-01_17.47.17'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJNOv3cVvJ68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab72d0bb-39b5-4ac2-b24f-a7addaf2ce09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mare-shevidi\u001b[0m (\u001b[33mare-shevidi-startup\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "# Log in to Weights & Biases\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure Weights & Biases to record against our project\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fri9R0rputN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvXVoJH8LS6u"
      },
      "outputs": [],
      "source": [
        "#dataset = load_dataset(DATASET_NAME)\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ArezoSh2021/ieee1547-qa\")\n",
        "\n",
        "# Split the training data into train and test sets (90/10 split)\n",
        "train_test_split = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "train = train_test_split['train']\n",
        "test = train_test_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you wish to reduce the training dataset to 20,000 points instead, then uncomment this line:\n",
        "# train = train.select(range(20000))"
      ],
      "metadata": {
        "id": "rJb9IDVjOAn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_SUsKqA23Gc"
      },
      "outputs": [],
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## Now load the Tokenizer and Model\n",
        "\n",
        "The model is \"quantized\" - we are reducing the precision to 4 bits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lb7M9xn46wx"
      },
      "outputs": [],
      "source": [
        "# pick the right quantization\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    device_map=\"auto\" # Add device_map=\"auto\" here\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\" # Add device_map=\"auto\" here\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# Load the Tokenizer and the Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"balanced\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2omVEaPIVJZa"
      },
      "outputs": [],
      "source": [
        "#from trl import DataCollatorForCompletionOnlyLM\n",
        "#response_template = \"Price is $\"\n",
        "#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AND NOW\n",
        "\n",
        "## We set up the configuration for Training\n",
        "\n",
        "We need to create 2 objects:\n",
        "\n",
        "A LoraConfig object with our hyperparameters for LoRA\n",
        "\n",
        "An SFTConfig with our overall Training parameters"
      ],
      "metadata": {
        "id": "4DaOeBhyy9eS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCwmDmkSATvj"
      },
      "outputs": [],
      "source": [
        "# First, specify the configuration parameters for LoRA\n",
        "\n",
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")\n",
        "\n",
        "# Next, specify the general configuration parameters for training\n",
        "\n",
        "train_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_strategy=\"no\",\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    dataset_text_field=\"text\", # Keep this as 'text' as we'll create this column\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True\n",
        ")\n",
        "\n",
        "# Function to combine instruction, input, and output into a single text field\n",
        "def combine_text(examples):\n",
        "    # Format the text as you would for the model\n",
        "    examples['text'] = [\n",
        "        f\"Instruction: {instruction}\\nInput: {input}\\nOutput: {output}\"\n",
        "        for instruction, input, output in zip(examples['instruction'], examples['input'], examples['output'])\n",
        "    ]\n",
        "    return examples\n",
        "\n",
        "# Apply the function to create the 'text' column\n",
        "train = train.map(combine_text, batched=True)\n",
        "\n",
        "\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train,\n",
        "    peft_config=lora_parameters,\n",
        "    args=train_parameters,\n",
        "    # data_collator=collator # Add the data collator here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune!\n",
        "fine_tuning.train()\n",
        "\n",
        "# Push our fine-tuned model to Hugging Face\n",
        "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ],
      "metadata": {
        "id": "GfvAxnXPvB7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32vvrYRVAUNg"
      },
      "outputs": [],
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4065b068"
      },
      "source": [
        "print(train)\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall transformers==4.48.0\n"
      ],
      "metadata": {
        "id": "uZwzc4pU4RD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Model\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"ArezoSh2021/NeWgrid-2025-08-01_17.47.17\")\n",
        "#pipe = pipeline(\"text-generation\", model=\"ArezoSh2021/NeWgrid-2025-08-01_17.47.17\")\n",
        "\n",
        "pipe(\"Can the OpenDER model be used to analyze fault current contributions from DERs?\", max_new_tokens=80)\n"
      ],
      "metadata": {
        "id": "DoE6IBnC_WjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "js3wq0hSzhFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}